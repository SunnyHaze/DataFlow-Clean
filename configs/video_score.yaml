model_cache_path: '../ckpt' # Path to cache models
num_workers: 2

data:
  # video:
  #   meta_data_path: '/mnt/petrelfs/mengzimo/videodataset/pure/metadata/KoNViD_1k_attributes.csv' # Path to meta data (mainly for image or video data)
  #   data_path: '/mnt/hwfile/opendatalab/mengzimo/KoNViD_1k_videos/' # Path to dataset
  #   # meta_data_path: '/mnt/petrelfs/mengzimo/videodataset/vatex/vatex_test/test-00000-of-00001.parquet'
  #   # data_path: '/mnt/petrelfs/mengzimo/videodataset/vatex/videos/'
  #   formatter: 'PureVideoFormatter' # formatter for pure video evaluation
  video-caption:
    meta_data_path: '/mnt/petrelfs/mengzimo/videodataset/vatex/vatex_test/test-00000-of-00001.parquet'
    data_path: '/mnt/petrelfs/mengzimo/videodataset/vatex/videos/'
    formatter: 'VideoCaptionFormatter'  # formatter for video-caption evaluation

scorers:
  # VideoMotionScorer:                              # Keep samples with video motion scores within a specific range.
  #     batch_size: 1
  #     num_workers: 4
  #     min_score: 0.25                                         # the minimum motion score to keep samples
  #     max_score: 10000.0                                      # the maximum motion score to keep samples
  #     sampling_fps: 2                                         # the samplig rate of frames_per_second to compute optical flow
  #     size: null                                              # resize frames along the smaller edge before computing optical flow, or a sequence like (h, w)
  #     max_size: null                                          # maximum allowed for the longer edge of resized frames
  #     relative: false                                         # whether to normalize the optical flow magnitude to [0, 1], relative to the frame's diagonal length
  #     any_or_all: any                                         # keep this sample when any/all videos meet the filter condition
  # FastVQAScorer:
  #     model_path: ./models/FAST_VQA_B_1_4.pth
  #     batch_size: 4
  #     num_workers: 4
  #     model_args:
  #       backbone:
  #         fragments:
  #           checkpoint: false
  #           pretrained: 
  #       backbone_size: swin_tiny_grpb
  #       backbone_preserve_keys: fragments
  #       divide_head: false
  #       vqa_head:
  #         in_channels: 768
  #         hidden_channels: 64
  #     fragments:
  #       fragments_h: 7
  #       fragments_w: 7
  #       fsize_h: 32
  #       fsize_w: 32
  #       aligned: 32
  #       clip_len: 32
  #       frame_interval: 2
  #       num_clips: 4
  # FasterVQAScorer:
  #     model_path: ./models/FAST_VQA_3D_1_1.pth
  #     batch_size: 4
  #     num_workers: 4
  #     model_args:
  #       backbone:
  #           fragments:
  #               checkpoint: false
  #               pretrained: 
  #       backbone_size: swin_tiny_grpb
  #       backbone_preserve_keys: fragments
  #       divide_head: false
  #       vqa_head:
  #           in_channels: 768
  #           hidden_channels: 64
  #     fragments:
  #       fragments_h: 7 #7
  #       fragments_w: 7 #7
  #       fsize_h: 32
  #       fsize_w: 32
  #       aligned: 8
  #       clip_len: 32 #32
  #       frame_interval: 2
  #       t_frag: 8 #8
  #       num_clips: 1
  # DOVERScorer:
  #     model_path: ./models/DOVER.pth
  #     batch_size: 4
  #     num_workers: 4
  #     model_args:
  #       backbone:
  #         technical:
  #           type: swin_tiny_grpb
  #           checkpoint: true
  #           pretrained:
  #         aesthetic:
  #           type: conv_tiny
  #       backbone_preserve_keys: technical,aesthetic
  #       divide_head: true
  #       vqa_head:
  #         in_channels: 768
  #         hidden_channels: 64
  #     sample_types:
  #       technical:
  #         fragments_h: 7
  #         fragments_w: 7
  #         fsize_h: 32
  #         fsize_w: 32
  #         aligned: 32
  #         clip_len: 32
  #         frame_interval: 2
  #         num_clips: 3
  #       aesthetic:
  #         size_h: 224
  #         size_w: 224
  #         clip_len: 32
  #         frame_interval: 2
  #         t_frag: 32
  #         num_clips: 1
  EMScorer:
    batch_size: 16
    num_workers: 4
  PACScorer:
    batch_size: 16
    num_workers: 4
    model_path: ./models/clip_ViT-B-32.pth


