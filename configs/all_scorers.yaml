# We list all scorers of three modalities here
# Please paste the selected scorers into the corresponding configuration file when using

text:
  AlpagasusScorer:
      api_key: 'Your Key'
      model: 'gpt-3.5-turbo'
      dimension: 'quality'
  PerspectiveScorer:
      api_key: 'Your Key'
      api_name: 'commentanalyzer'
      api_version: 'v1alpha1'
      discovery_service_url: 'https://commentanalyzer.googleapis.com/$discovery/rest?version=v1alpha1'
      static_discovery: False
  TreeinstructScorer:
      api_key: 'Your Key'
      model: 'gpt-3.5-turbo'
  LexicalDiversityScorer:
      metrics_to_keep:
        mtld: True
        hdd: True
  NgramScorer:
      ngrams: 5
  LangkitScorer:
      metrics_to_keep:
        flesch_reading_ease: True
        automated_readability_index: True
        aggregate_reading_level: True
        syllable_count: True
        lexicon_count: True
        sentence_count: True
        character_count: True
        letter_count: True
        polysyllable_count: True
        monosyllable_count: True
        difficult_words: True
  UnievalScorer:
      device: 'cuda:0'
      metrics_to_keep:
        fluency: True
        naturalness: True
        understandability: True
  InstagScorer:
      model_path: 'OFA-Sys/InsTagger'
      temperature: 0
      do_sample: False
      max_new_tokens: 2048
      num_return_sequences: 1
      return_dict_in_generate: True
      device: 'cuda:0'
  TextbookScorer: # cpu(fasttext)
      model_repo: 'kenhktsui/llm-data-textbook-quality-fasttext-classifer-v2'
      model_file: 'model.bin'
      low_score: 0
      mid_score: 1
      high_score: 2
      batch_size: 8
  FineWebEduScorer:
      model_name: 'HuggingFaceTB/fineweb-edu-classifier'
      device: 'cuda:0'
  VendiScorer:
      bert_model_path: 'bert-base-uncased'
      simcse_model_path: 'princeton-nlp/unsup-simcse-bert-base-uncased'
      device: 'cuda:0'
      metrics_to_keep: 
        ngram: True
        bert: True
        simcse: True
  Task2VecScorer:
      sample_nums: 2 # sampling times
      sample_size: 8 # batch size for one sample
      device: 'cuda:0'
      method: 'montecarlo' # from [montecarlo, variational]
  DebertaV3Scorer:
      model_name: 'nvidia/quality-classifier-deberta'
      device: 'cuda:0'
      batch_size: 16
  PerplexityScorer: # cpu(Kenlm)
      model_path: 'dataflow/Eval/Text/models/Kenlm/wikipedia' # please refer to https://huggingface.co/edugp/kenlm/tree/main and download the corresponding model, such as wikipedia
      language: 'en'
  QuratingScorer:
      model: 'princeton-nlp/QuRater-1.3B'
      tokens_field: 'input_ids'
      tokens: 512
      map_batch_size: 512
      num_workers: 1
      device_batch_size: 16
      device: 'cuda:0'
      labels:
        - writing_style
        - required_expertise
        - facts_and_trivia
        - educational_value
  SuperfilteringScorer:
      device: 'cuda:0'
      model_name: 'gpt2'
      prompt: 'none'
      max_length: 1024
  DeitaQualityScorer:
      device: 'cuda:0'
      model_name: 'hkust-nlp/deita-quality-scorer'
      max_length: 512
  DeitaComplexityScorer:
    device: 'cuda:0' 
    model_name: 'hkust-nlp/deita-complexity-scorer' 
    max_length: 512
  PresidioScorer:
      language: 'en'
      device: 'cuda:0'
  RMScorer:
    model_name: 'OpenAssistant/reward-model-deberta-v3-large-v2'  
    device: 'cuda:0' 
    batch_size: 64 


image:
  clip:
    class_name: ClipScorer
    data_type: image_caption
    batch_size: 2
  longclip:
    class_name: LongClipScorer
    data_type: image_caption
    model_size: B # For larger models, use L
    batch_size: 2
  vqa_score:
    class_name: ClipT5Scorer
    model_size: xl # For larger models, use xxl
    data_type: image_caption
    # model_cache_folder: /clip_t5
    context_len: 2048
    batch_size: 2
    # system_msg: "A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions."
    # ignore_index: -100
    # image_token_index: -200
    # default_image_token: "<image>"
  fleur:
    class_name: FleurScorer
    data_type: image_caption
    model_size: 7 # For larger LLaVA model, use 13
    batch_size: 2
  pyiqa:
    class_name: PyiqaScorer
    data_type: image
    batch_size: 2
  fid_score:
    class_name: FIDScorer
    batch_size: 50
    num_workers: 8
    dims: 2048
    model: https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth
    device: cpu
    data_type: image
  kid_score:
    class_name: KIDScorer
    batch_size: 50
    num_workers: 8
    dims: 2048
    model:  inception
    device: cpu
    data_type: image
  is_score:
    class_name: ISScorer
    batch_size: 32
    cuda: True
    resize: True
    splits: 10
    data_type: image

  
video: # will be loaded as a list of dict, dict keys is model name, value is a dict of args.
  VideoMotionScorer:                              # Keep samples with video motion scores within a specific range.
      min_score: 0.25                                         # the minimum motion score to keep samples
      max_score: 10000.0                                      # the maximum motion score to keep samples
      sampling_fps: 2                                         # the samplig rate of frames_per_second to compute optical flow
      size: null                                              # resize frames along the smaller edge before computing optical flow, or a sequence like (h, w)
      max_size: null                                          # maximum allowed for the longer edge of resized frames
      relative: false                                         # whether to normalize the optical flow magnitude to [0, 1], relative to the frame's diagonal length
      any_or_all: any                                         # keep this sample when any/all videos meet the filter condition
  FastVQAScorer:
      model_path: /mnt/hwfile/opendatalab/mengzimo/models/FAST_VQA_B_1_4.pth
      batch_size: 4
      num_workers: 4
      model_args:
        backbone:
          fragments:
            checkpoint: false
            pretrained: 
        backbone_size: swin_tiny_grpb
        backbone_preserve_keys: fragments
        divide_head: false
        vqa_head:
          in_channels: 768
          hidden_channels: 64
      fragments:
        fragments_h: 7
        fragments_w: 7
        fsize_h: 32
        fsize_w: 32
        aligned: 32
        clip_len: 32
        frame_interval: 2
        num_clips: 4
  FasterVQAScorer:
      model_path: /mnt/hwfile/opendatalab/mengzimo/models/FAST_VQA_3D_1_1.pth
      batch_size: 4
      num_workers: 4
      model_args:
        backbone:
            fragments:
                checkpoint: false
                pretrained: 
        backbone_size: swin_tiny_grpb
        backbone_preserve_keys: fragments
        divide_head: false
        vqa_head:
            in_channels: 768
            hidden_channels: 64
      fragments:
        fragments_h: 7 #7
        fragments_w: 7 #7
        fsize_h: 32
        fsize_w: 32
        aligned: 8
        clip_len: 32 #32
        frame_interval: 2
        t_frag: 8 #8
        num_clips: 1
  DOVERScorer:
      model_path: /mnt/hwfile/opendatalab/mengzimo/models/DOVER.pth
      batch_size: 4
      num_workers: 4
      model_args:
        backbone:
          technical:
            type: swin_tiny_grpb
            checkpoint: true
            pretrained:
          aesthetic:
            type: conv_tiny
        backbone_preserve_keys: technical,aesthetic
        divide_head: true
        vqa_head:
          in_channels: 768
          hidden_channels: 64
      sample_types:
        technical:
          fragments_h: 7
          fragments_w: 7
          fsize_h: 32
          fsize_w: 32
          aligned: 32
          clip_len: 32
          frame_interval: 2
          num_clips: 3
        aesthetic:
          size_h: 224
          size_w: 224
          clip_len: 32
          frame_interval: 2
          t_frag: 32
          num_clips: 1
  EMScorer:
    batch_size: 4
    num_workers: 4

